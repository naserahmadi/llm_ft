{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/test_qa.json', 'r') as f:\n",
    "    test_qa = json.load(f)\n",
    "\n",
    "with open('data/test_sum.json', 'r') as f:\n",
    "    test_sum = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n",
      "/home/user01/naser-workspace/DIABOLOCOM/llm_ft/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/user01/naser-workspace/DIABOLOCOM/llm_ft/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_folder = \"llama2-7b-chat-ft/checkpoint-40/\"\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_folder,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt, task):\n",
    "    if task == 'qa':\n",
    "        real = prompt['instruction'].split('### Answer:\\n')[1].replace('</s>','').strip()\n",
    "        text = prompt['instruction'].split('### Answer:\\n')[0] + '### Answer:\\n'\n",
    "    elif task == 'sum':\n",
    "        real = prompt['instruction'].split('### Summary:\\n')[1].replace('</s>','').strip()\n",
    "        text = prompt['instruction'].split('### Summary:\\n')[0] + '### Summary:\\n'\n",
    "\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "    if task == 'qa':\n",
    "        return {'real': real,\n",
    "                'pred': \n",
    "                tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0].split('### Answer:\\n')[1]\n",
    "        }\n",
    "    elif task == 'sum':\n",
    "        return {'real': real,\n",
    "                'pred':   \n",
    "                tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0].split('### Summary:\\n')[1]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/naser-workspace/DIABOLOCOM/llm_ft/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'real': \"Beth wants to organize a girls weekend to celebrate her mother's 40th birthday. She also wants to work at Deidre's beauty salon. Deidre offers her a few hours on Saturdays as work experience. They set up for a meeting tomorrow.\",\n",
       " 'pred': \"Beth wants to try working in the salon for a few weeks as work experience. She'll meet Maxine tomorrow.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(test_sum[20], task='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '<s>### Instruction:\\nYou are a helpful, respectful and honest assistant. Your task is to answer the following question. Your answer should be based on the provided context only.\\n\\n### Question:\\nWho had an 80s No 1 hit with Hold On To The Nights?\\n\\n### Context:\\n[DOC] [TLE] Hold On to the Nights\"Hold On to the Nights\" is a power ballad and number-one hit for American rock singer/songwriter/musician Richard Marx.  This was the fourth and final single released from his self-titled debut album, and the first to reach the top of the Billboard Hot 100 chart. [PAR] Written by Richard Marx, \"Hold On to the Nights\" reached the Billboard Hot 100 number 1 position on July 23, 1988, preventing Def Leppard\\'s \"Pour Some Sugar On Me\" from reaching the top spot that same week. The song was on the chart for twenty-one weeks, and left the chart at number 65. From Marx\\' debut 1987 album, Richard Marx, the song also reached number three on the Billboard Adult Contemporary chart. \"Hold On to the Nights\" has been re-released numerous albums  and is included on Marx\\'s live performance DVD A Night Out with Friends (2012). [PAR] Charts\\n\\n\\n### Answer:\\nrichard marx </s>'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
